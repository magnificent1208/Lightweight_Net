# 剪枝 pruning
《Learning both weights and connection for efficient neural networks》

## 1 研究背景
- FLOPS（G）每秒浮点运算次数  **区分FLOPS/FLOPs/TOPS

  | Model     | FLOPS(G) |
  | --------- | -------- |
  | VGG-16    | 15       |
  | ResNet-50 | 4.1      |
  | YOLOv3    | 328      |

  | Device       | FLOPS(G) |
  | ------------ | -------- |
  | NIVIDIA nano | 472      |
  | NVIDIA TX2   | 1500     |
  
- 动机1：通过训练改变模型结构，减少参数量，降低模型计算量，加速推理速度。[main]

- 动机2： 模型通过训练学习到的权重，可以作为判断神经元重要性的指标。

- 动机3：同时训练权重和要剪的神经元可以减少损失。

## 2 参数量和计算量

2.1 参数量
2.1.1 定义：模型所有带参数的层的权重的总和。主要有 卷积层、BN层、全连接层。
2.1.1.1 卷积层计算量 Kernel_height*K_width*C_input*C_output + C_output/bias
2.1.1.2 BN层计算量  2*C_input （因为没有尺度变换 输入输出一样）
2.1.1.3 全连接层计算量 C_input *Coutput + C_output/bias

2.2 FLOPS 
2.2.1 定义：每秒浮点运算次数/每秒峰值速度。用于估计处理器的执行效能。
2.2.1.1 卷积层计算量 
2.2.1.2 全连接层计算量 

## 模型剪枝方法综述()
1. SVD和量化。
2. 用池化层代替全连接层。
3. 其他剪枝方法。如基于海森矩阵。
## 本文方法：三步骤
S1 在数据集上对未压缩的模型进行完整的训练，直至收敛。
S2 在经过完整训练的未压缩模型基础上，减掉一批权重小于阈值的神经元。
S3 在数据集上对剪过后的模型进行权重微调，使其恢复精度

## 讨论正则化对剪枝过程的影响
### L1
### L2 
## 结果
约10倍的压缩

## 代码复现